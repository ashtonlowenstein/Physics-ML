{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-23T21:37:23.670531Z",
     "start_time": "2026-02-23T21:37:22.990714Z"
    }
   },
   "source": [
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T22:38:09.446914Z",
     "start_time": "2026-02-23T22:38:09.418310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FNOLayer(nn.Module):\n",
    "    def __init__(self, features: int, k_max: int):\n",
    "        super().__init__()\n",
    "        self.features = features\n",
    "        self.k_max = k_max\n",
    "        #Fourier space filters\n",
    "        self.filter = nn.Parameter(tc.randn(features, features, k_max, dtype=tc.cfloat))\n",
    "\n",
    "        #Physical space transformation\n",
    "        self.linear_trans = nn.Linear(features, features)\n",
    "\n",
    "    def forward(self, x: tc.Tensor) -> tc.Tensor:\n",
    "        # x: (batch, grid_size, features), represents the function which solves desired diff eq\n",
    "        batch, grid_size, features = x.shape\n",
    "        x_tilde = tc.fft.rfft(x, dim = 1)[:,:self.k_max,:]\n",
    "        # x_tilde: (batch, k_max, features)\n",
    "        x_filtered = tc.einsum('bkf,fgk->bkg', x_tilde, self.filter)\n",
    "        x_tilde_full = tc.zeros(batch, grid_size//2 + 1, features, dtype=tc.cfloat, device=x.device)\n",
    "        x_tilde_full[:, :self.k_max, :] = x_filtered\n",
    "        freq_branch = tc.fft.irfft(\n",
    "            x_tilde_full,\n",
    "            n=grid_size,\n",
    "            dim = 1).real\n",
    "        space_branch = self.linear_trans(x)\n",
    "        output = space_branch + freq_branch\n",
    "        return nn.GELU()(output)"
   ],
   "id": "badbf30dee39689a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T23:38:25.504581Z",
     "start_time": "2026-02-23T23:38:25.476601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FNO(nn.Module):\n",
    "    def __init__(self, features: int, k_max: int, num_layers: int, grid_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # This can be included if we don't want the grid to be created dynamically in the forward call\n",
    "        # self.register_buffer('grid', tc.linspace(0, 1, grid_size).reshape(1, grid_size, 1))\n",
    "\n",
    "        self.input_proj = nn.Linear(2, features)\n",
    "        self.GNO_layers = nn.ModuleList([FNOLayer(features, k_max) for _ in range(num_layers)])\n",
    "        self.output_proj = nn.Linear(features, 1)\n",
    "\n",
    "    def forward(self, x: tc.Tensor) -> tc.Tensor:\n",
    "        batch, grid_size, _ = x.shape\n",
    "        grid = tc.linspace(0, 1, grid_size, device=x.device).reshape(1, grid_size, 1).expand(batch, -1, -1)\n",
    "        x = tc.cat([x, grid], dim=-1)\n",
    "        x = self.input_proj(x)\n",
    "        for layer in self.GNO_layers:\n",
    "            x = layer(x)\n",
    "        x = self.output_proj(x)\n",
    "        return x.squeeze(-1)"
   ],
   "id": "7b60ad4f80415131",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T22:38:13.876158Z",
     "start_time": "2026-02-23T22:38:13.850488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def random_initial_condition(x, n_modes=5):\n",
    "    u0 = np.zeros_like(x)\n",
    "    amps = []\n",
    "    for k in range(1, n_modes + 1):\n",
    "        amplitude = np.random.uniform(-1, 1)\n",
    "        u0 += amplitude * np.sin(k * np.pi * x)\n",
    "        amps.append(amplitude)\n",
    "    return u0, amps"
   ],
   "id": "a527401fbfa6c8af",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T22:38:16.979982Z",
     "start_time": "2026-02-23T22:38:16.544302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "def solve_heat_equation(u0, x, t_end=0.1, alpha=1.0):\n",
    "    N = len(x)\n",
    "    dx = x[1] - x[0]\n",
    "\n",
    "    def heat_rhs(t, u):\n",
    "        dudt = np.zeros_like(u)\n",
    "        # finite difference for second derivative\n",
    "        dudt[1:-1] = alpha * (u[2:] - 2*u[1:-1] + u[:-2]) / dx**2\n",
    "        # boundary conditions: u[0] = u[-1] = 0 (already zero)\n",
    "        return dudt\n",
    "\n",
    "    sol = solve_ivp(heat_rhs, [0, t_end], u0, method='RK45',\n",
    "                    t_eval=[t_end], rtol=1e-6, atol=1e-8)\n",
    "    return sol.y[:, -1]  # solution at t_end"
   ],
   "id": "693d5aa6d385e7ce",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T22:41:38.100849Z",
     "start_time": "2026-02-23T22:38:24.706849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N = 64          # spatial grid points during training\n",
    "x = np.linspace(0, 1, N)\n",
    "n_samples = 10000\n",
    "\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    u0, _ = random_initial_condition(x)\n",
    "    uT = solve_heat_equation(u0, x)\n",
    "    inputs.append(u0)\n",
    "    targets.append(uT)\n",
    "\n",
    "inputs = np.array(inputs)   # (10000, 64)\n",
    "targets = np.array(targets) # (10000, 64)\n",
    "np.save('inputs.npy', inputs)\n",
    "np.save('targets.npy', targets)"
   ],
   "id": "f45c7c2d74fb9a28",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T22:43:09.447824Z",
     "start_time": "2026-02-23T22:43:09.416257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Don't need to load if calculating the data for the first time\n",
    "# inputs = np.load('inputs.npy')\n",
    "# targets = np.load('targets.npy')\n",
    "X = tc.tensor(inputs, dtype=tc.float32).unsqueeze(-1)  # (10000, 64, 1)\n",
    "Y = tc.tensor(targets, dtype=tc.float32)               # (10000, 64)\n",
    "\n",
    "dataset = TensorDataset(X, Y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_set, val_set = tc.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=64)"
   ],
   "id": "b78a4b772d3900a6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T23:54:43.873704Z",
     "start_time": "2026-02-23T23:39:05.894861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = FNO(features=64, k_max=16, num_layers=4, grid_size=64)\n",
    "optimizer = tc.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "scheduler = tc.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "\n",
    "for epoch in range(350):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with tc.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            pred = model(x_batch)\n",
    "            val_loss += loss_fn(pred, y_batch).item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss/len(train_loader):.6f} | Val Loss: {val_loss/len(val_loader):.6f}\")\n",
    "    scheduler.step(val_loss)"
   ],
   "id": "901aae0ad1a3a12e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 1624.555689 | Val Loss: 301.651521\n",
      "Epoch 2 | Train Loss: 212.039833 | Val Loss: 152.829993\n",
      "Epoch 3 | Train Loss: 119.874609 | Val Loss: 97.364727\n",
      "Epoch 4 | Train Loss: 80.128402 | Val Loss: 69.052670\n",
      "Epoch 5 | Train Loss: 57.990014 | Val Loss: 51.747051\n",
      "Epoch 6 | Train Loss: 44.259965 | Val Loss: 40.640850\n",
      "Epoch 7 | Train Loss: 34.975215 | Val Loss: 32.749330\n",
      "Epoch 8 | Train Loss: 28.379397 | Val Loss: 26.969306\n",
      "Epoch 9 | Train Loss: 23.486779 | Val Loss: 22.683391\n",
      "Epoch 10 | Train Loss: 19.720148 | Val Loss: 19.163158\n",
      "Epoch 11 | Train Loss: 16.763180 | Val Loss: 16.499756\n",
      "Epoch 12 | Train Loss: 14.387924 | Val Loss: 14.247454\n",
      "Epoch 13 | Train Loss: 12.443971 | Val Loss: 12.421850\n",
      "Epoch 14 | Train Loss: 10.851373 | Val Loss: 10.907535\n",
      "Epoch 15 | Train Loss: 9.519711 | Val Loss: 9.628162\n",
      "Epoch 16 | Train Loss: 8.384711 | Val Loss: 8.489716\n",
      "Epoch 17 | Train Loss: 7.412732 | Val Loss: 7.561955\n",
      "Epoch 18 | Train Loss: 6.583508 | Val Loss: 6.739882\n",
      "Epoch 19 | Train Loss: 5.866815 | Val Loss: 6.028970\n",
      "Epoch 20 | Train Loss: 5.239997 | Val Loss: 5.433208\n",
      "Epoch 21 | Train Loss: 4.690089 | Val Loss: 4.860869\n",
      "Epoch 22 | Train Loss: 4.214300 | Val Loss: 4.370281\n",
      "Epoch 23 | Train Loss: 3.779293 | Val Loss: 3.947038\n",
      "Epoch 24 | Train Loss: 3.404631 | Val Loss: 3.561591\n",
      "Epoch 25 | Train Loss: 3.071483 | Val Loss: 3.221585\n",
      "Epoch 26 | Train Loss: 2.768641 | Val Loss: 2.909206\n",
      "Epoch 27 | Train Loss: 2.499069 | Val Loss: 2.632967\n",
      "Epoch 28 | Train Loss: 2.261604 | Val Loss: 2.383889\n",
      "Epoch 29 | Train Loss: 2.046155 | Val Loss: 2.156167\n",
      "Epoch 30 | Train Loss: 1.849499 | Val Loss: 1.954973\n",
      "Epoch 31 | Train Loss: 1.675052 | Val Loss: 1.780321\n",
      "Epoch 32 | Train Loss: 1.516367 | Val Loss: 1.613953\n",
      "Epoch 33 | Train Loss: 1.373379 | Val Loss: 1.458415\n",
      "Epoch 34 | Train Loss: 1.242534 | Val Loss: 1.321615\n",
      "Epoch 35 | Train Loss: 1.124742 | Val Loss: 1.197886\n",
      "Epoch 36 | Train Loss: 1.017883 | Val Loss: 1.083083\n",
      "Epoch 37 | Train Loss: 0.920633 | Val Loss: 0.980431\n",
      "Epoch 38 | Train Loss: 0.831351 | Val Loss: 0.887332\n",
      "Epoch 39 | Train Loss: 0.751195 | Val Loss: 0.802959\n",
      "Epoch 40 | Train Loss: 0.677619 | Val Loss: 0.720127\n",
      "Epoch 41 | Train Loss: 0.611874 | Val Loss: 0.651565\n",
      "Epoch 42 | Train Loss: 0.550613 | Val Loss: 0.585703\n",
      "Epoch 43 | Train Loss: 0.494876 | Val Loss: 0.527494\n",
      "Epoch 44 | Train Loss: 0.444607 | Val Loss: 0.473417\n",
      "Epoch 45 | Train Loss: 0.398418 | Val Loss: 0.422489\n",
      "Epoch 46 | Train Loss: 0.356958 | Val Loss: 0.380541\n",
      "Epoch 47 | Train Loss: 0.319085 | Val Loss: 0.337894\n",
      "Epoch 48 | Train Loss: 0.284541 | Val Loss: 0.300914\n",
      "Epoch 49 | Train Loss: 0.253257 | Val Loss: 0.267933\n",
      "Epoch 50 | Train Loss: 0.224878 | Val Loss: 0.236753\n",
      "Epoch 51 | Train Loss: 0.199278 | Val Loss: 0.209581\n",
      "Epoch 52 | Train Loss: 0.175827 | Val Loss: 0.184951\n",
      "Epoch 53 | Train Loss: 0.154747 | Val Loss: 0.162315\n",
      "Epoch 54 | Train Loss: 0.136037 | Val Loss: 0.142781\n",
      "Epoch 55 | Train Loss: 0.118921 | Val Loss: 0.124072\n",
      "Epoch 56 | Train Loss: 0.103632 | Val Loss: 0.108170\n",
      "Epoch 57 | Train Loss: 0.090075 | Val Loss: 0.093498\n",
      "Epoch 58 | Train Loss: 0.077954 | Val Loss: 0.080726\n",
      "Epoch 59 | Train Loss: 0.067078 | Val Loss: 0.069685\n",
      "Epoch 60 | Train Loss: 0.057600 | Val Loss: 0.059188\n",
      "Epoch 61 | Train Loss: 0.049138 | Val Loss: 0.050562\n",
      "Epoch 62 | Train Loss: 0.041798 | Val Loss: 0.042786\n",
      "Epoch 63 | Train Loss: 0.035383 | Val Loss: 0.036130\n",
      "Epoch 64 | Train Loss: 0.029915 | Val Loss: 0.030497\n",
      "Epoch 65 | Train Loss: 0.025194 | Val Loss: 0.025541\n",
      "Epoch 66 | Train Loss: 0.021201 | Val Loss: 0.021373\n",
      "Epoch 67 | Train Loss: 0.017886 | Val Loss: 0.018116\n",
      "Epoch 68 | Train Loss: 0.015182 | Val Loss: 0.015729\n",
      "Epoch 69 | Train Loss: 0.013069 | Val Loss: 0.013309\n",
      "Epoch 70 | Train Loss: 0.011306 | Val Loss: 0.011535\n",
      "Epoch 71 | Train Loss: 0.009992 | Val Loss: 0.010056\n",
      "Epoch 72 | Train Loss: 0.008893 | Val Loss: 0.009025\n",
      "Epoch 73 | Train Loss: 0.008028 | Val Loss: 0.008497\n",
      "Epoch 74 | Train Loss: 0.007330 | Val Loss: 0.007494\n",
      "Epoch 75 | Train Loss: 0.006826 | Val Loss: 0.007487\n",
      "Epoch 76 | Train Loss: 0.006259 | Val Loss: 0.006662\n",
      "Epoch 77 | Train Loss: 0.006082 | Val Loss: 0.006101\n",
      "Epoch 78 | Train Loss: 0.005689 | Val Loss: 0.006308\n",
      "Epoch 79 | Train Loss: 0.005333 | Val Loss: 0.005381\n",
      "Epoch 80 | Train Loss: 0.004922 | Val Loss: 0.005512\n",
      "Epoch 81 | Train Loss: 0.004685 | Val Loss: 0.005079\n",
      "Epoch 82 | Train Loss: 0.004424 | Val Loss: 0.005185\n",
      "Epoch 83 | Train Loss: 0.004398 | Val Loss: 0.004466\n",
      "Epoch 84 | Train Loss: 0.004055 | Val Loss: 0.004324\n",
      "Epoch 85 | Train Loss: 0.003835 | Val Loss: 0.003843\n",
      "Epoch 86 | Train Loss: 0.003689 | Val Loss: 0.003757\n",
      "Epoch 87 | Train Loss: 0.003664 | Val Loss: 0.003501\n",
      "Epoch 88 | Train Loss: 0.004062 | Val Loss: 0.003314\n",
      "Epoch 89 | Train Loss: 0.003679 | Val Loss: 0.004992\n",
      "Epoch 90 | Train Loss: 0.003689 | Val Loss: 0.003268\n",
      "Epoch 91 | Train Loss: 0.004012 | Val Loss: 0.003533\n",
      "Epoch 92 | Train Loss: 0.004615 | Val Loss: 0.011509\n",
      "Epoch 93 | Train Loss: 0.005447 | Val Loss: 0.002883\n",
      "Epoch 94 | Train Loss: 0.005657 | Val Loss: 0.003531\n",
      "Epoch 95 | Train Loss: 0.003350 | Val Loss: 0.014500\n",
      "Epoch 96 | Train Loss: 0.005445 | Val Loss: 0.002559\n",
      "Epoch 97 | Train Loss: 0.004048 | Val Loss: 0.009556\n",
      "Epoch 98 | Train Loss: 0.006774 | Val Loss: 0.003987\n",
      "Epoch 99 | Train Loss: 0.003605 | Val Loss: 0.003414\n",
      "Epoch 100 | Train Loss: 0.005492 | Val Loss: 0.004078\n",
      "Epoch 101 | Train Loss: 0.004101 | Val Loss: 0.006074\n",
      "Epoch 102 | Train Loss: 0.003516 | Val Loss: 0.002431\n",
      "Epoch 103 | Train Loss: 0.005804 | Val Loss: 0.001979\n",
      "Epoch 104 | Train Loss: 0.002644 | Val Loss: 0.002880\n",
      "Epoch 105 | Train Loss: 0.004493 | Val Loss: 0.005551\n",
      "Epoch 106 | Train Loss: 0.005120 | Val Loss: 0.009137\n",
      "Epoch 107 | Train Loss: 0.002788 | Val Loss: 0.001708\n",
      "Epoch 108 | Train Loss: 0.003243 | Val Loss: 0.004936\n",
      "Epoch 109 | Train Loss: 0.004012 | Val Loss: 0.002942\n",
      "Epoch 110 | Train Loss: 0.003101 | Val Loss: 0.001628\n",
      "Epoch 111 | Train Loss: 0.004030 | Val Loss: 0.002388\n",
      "Epoch 112 | Train Loss: 0.003236 | Val Loss: 0.003188\n",
      "Epoch 113 | Train Loss: 0.003015 | Val Loss: 0.001359\n",
      "Epoch 114 | Train Loss: 0.003554 | Val Loss: 0.004974\n",
      "Epoch 115 | Train Loss: 0.002426 | Val Loss: 0.002053\n",
      "Epoch 116 | Train Loss: 0.003140 | Val Loss: 0.010276\n",
      "Epoch 117 | Train Loss: 0.002600 | Val Loss: 0.004867\n",
      "Epoch 118 | Train Loss: 0.002940 | Val Loss: 0.002572\n",
      "Epoch 119 | Train Loss: 0.002365 | Val Loss: 0.003517\n",
      "Epoch 120 | Train Loss: 0.001132 | Val Loss: 0.001213\n",
      "Epoch 121 | Train Loss: 0.000946 | Val Loss: 0.000983\n",
      "Epoch 122 | Train Loss: 0.000933 | Val Loss: 0.000936\n",
      "Epoch 123 | Train Loss: 0.000890 | Val Loss: 0.000899\n",
      "Epoch 124 | Train Loss: 0.000870 | Val Loss: 0.001029\n",
      "Epoch 125 | Train Loss: 0.000853 | Val Loss: 0.000848\n",
      "Epoch 126 | Train Loss: 0.000919 | Val Loss: 0.000951\n",
      "Epoch 127 | Train Loss: 0.000870 | Val Loss: 0.000833\n",
      "Epoch 128 | Train Loss: 0.000875 | Val Loss: 0.000767\n",
      "Epoch 129 | Train Loss: 0.000858 | Val Loss: 0.000748\n",
      "Epoch 130 | Train Loss: 0.001125 | Val Loss: 0.003274\n",
      "Epoch 131 | Train Loss: 0.001663 | Val Loss: 0.000869\n",
      "Epoch 132 | Train Loss: 0.001057 | Val Loss: 0.000763\n",
      "Epoch 133 | Train Loss: 0.001521 | Val Loss: 0.000767\n",
      "Epoch 134 | Train Loss: 0.000949 | Val Loss: 0.001500\n",
      "Epoch 135 | Train Loss: 0.001128 | Val Loss: 0.001367\n",
      "Epoch 136 | Train Loss: 0.000618 | Val Loss: 0.000680\n",
      "Epoch 137 | Train Loss: 0.000596 | Val Loss: 0.000611\n",
      "Epoch 138 | Train Loss: 0.000575 | Val Loss: 0.000600\n",
      "Epoch 139 | Train Loss: 0.000574 | Val Loss: 0.000606\n",
      "Epoch 140 | Train Loss: 0.000607 | Val Loss: 0.000566\n",
      "Epoch 141 | Train Loss: 0.000558 | Val Loss: 0.000595\n",
      "Epoch 142 | Train Loss: 0.000577 | Val Loss: 0.000569\n",
      "Epoch 143 | Train Loss: 0.000539 | Val Loss: 0.000567\n",
      "Epoch 144 | Train Loss: 0.000554 | Val Loss: 0.000738\n",
      "Epoch 145 | Train Loss: 0.000599 | Val Loss: 0.000510\n",
      "Epoch 146 | Train Loss: 0.000575 | Val Loss: 0.000553\n",
      "Epoch 147 | Train Loss: 0.000623 | Val Loss: 0.000727\n",
      "Epoch 148 | Train Loss: 0.000624 | Val Loss: 0.000480\n",
      "Epoch 149 | Train Loss: 0.000676 | Val Loss: 0.000678\n",
      "Epoch 150 | Train Loss: 0.000536 | Val Loss: 0.000446\n",
      "Epoch 151 | Train Loss: 0.000539 | Val Loss: 0.000533\n",
      "Epoch 152 | Train Loss: 0.000790 | Val Loss: 0.000622\n",
      "Epoch 153 | Train Loss: 0.000487 | Val Loss: 0.000736\n",
      "Epoch 154 | Train Loss: 0.000653 | Val Loss: 0.000881\n",
      "Epoch 155 | Train Loss: 0.000610 | Val Loss: 0.000417\n",
      "Epoch 156 | Train Loss: 0.000462 | Val Loss: 0.000426\n",
      "Epoch 157 | Train Loss: 0.000544 | Val Loss: 0.000368\n",
      "Epoch 158 | Train Loss: 0.000445 | Val Loss: 0.000348\n",
      "Epoch 159 | Train Loss: 0.000480 | Val Loss: 0.000382\n",
      "Epoch 160 | Train Loss: 0.000524 | Val Loss: 0.000602\n",
      "Epoch 161 | Train Loss: 0.000497 | Val Loss: 0.000339\n",
      "Epoch 162 | Train Loss: 0.000430 | Val Loss: 0.000361\n",
      "Epoch 163 | Train Loss: 0.000461 | Val Loss: 0.001192\n",
      "Epoch 164 | Train Loss: 0.000445 | Val Loss: 0.000985\n",
      "Epoch 165 | Train Loss: 0.000531 | Val Loss: 0.000307\n",
      "Epoch 166 | Train Loss: 0.000424 | Val Loss: 0.000447\n",
      "Epoch 167 | Train Loss: 0.000366 | Val Loss: 0.000306\n",
      "Epoch 168 | Train Loss: 0.000460 | Val Loss: 0.000257\n",
      "Epoch 169 | Train Loss: 0.000313 | Val Loss: 0.000499\n",
      "Epoch 170 | Train Loss: 0.000370 | Val Loss: 0.000239\n",
      "Epoch 171 | Train Loss: 0.000436 | Val Loss: 0.000276\n",
      "Epoch 172 | Train Loss: 0.000354 | Val Loss: 0.000320\n",
      "Epoch 173 | Train Loss: 0.000309 | Val Loss: 0.000482\n",
      "Epoch 174 | Train Loss: 0.000350 | Val Loss: 0.000231\n",
      "Epoch 175 | Train Loss: 0.000379 | Val Loss: 0.000362\n",
      "Epoch 176 | Train Loss: 0.000294 | Val Loss: 0.000197\n",
      "Epoch 177 | Train Loss: 0.000237 | Val Loss: 0.000464\n",
      "Epoch 178 | Train Loss: 0.000285 | Val Loss: 0.000243\n",
      "Epoch 179 | Train Loss: 0.000302 | Val Loss: 0.000197\n",
      "Epoch 180 | Train Loss: 0.000250 | Val Loss: 0.000343\n",
      "Epoch 181 | Train Loss: 0.000246 | Val Loss: 0.000740\n",
      "Epoch 182 | Train Loss: 0.000278 | Val Loss: 0.000313\n",
      "Epoch 183 | Train Loss: 0.000146 | Val Loss: 0.000150\n",
      "Epoch 184 | Train Loss: 0.000136 | Val Loss: 0.000137\n",
      "Epoch 185 | Train Loss: 0.000130 | Val Loss: 0.000131\n",
      "Epoch 186 | Train Loss: 0.000126 | Val Loss: 0.000130\n",
      "Epoch 187 | Train Loss: 0.000127 | Val Loss: 0.000125\n",
      "Epoch 188 | Train Loss: 0.000125 | Val Loss: 0.000159\n",
      "Epoch 189 | Train Loss: 0.000130 | Val Loss: 0.000168\n",
      "Epoch 190 | Train Loss: 0.000126 | Val Loss: 0.000116\n",
      "Epoch 191 | Train Loss: 0.000133 | Val Loss: 0.000226\n",
      "Epoch 192 | Train Loss: 0.000115 | Val Loss: 0.000106\n",
      "Epoch 193 | Train Loss: 0.000113 | Val Loss: 0.000101\n",
      "Epoch 194 | Train Loss: 0.000119 | Val Loss: 0.000101\n",
      "Epoch 195 | Train Loss: 0.000135 | Val Loss: 0.000108\n",
      "Epoch 196 | Train Loss: 0.000127 | Val Loss: 0.000094\n",
      "Epoch 197 | Train Loss: 0.000117 | Val Loss: 0.000096\n",
      "Epoch 198 | Train Loss: 0.000137 | Val Loss: 0.000118\n",
      "Epoch 199 | Train Loss: 0.000121 | Val Loss: 0.000105\n",
      "Epoch 200 | Train Loss: 0.000110 | Val Loss: 0.000086\n",
      "Epoch 201 | Train Loss: 0.000122 | Val Loss: 0.000120\n",
      "Epoch 202 | Train Loss: 0.000108 | Val Loss: 0.000272\n",
      "Epoch 203 | Train Loss: 0.000125 | Val Loss: 0.000221\n",
      "Epoch 204 | Train Loss: 0.000086 | Val Loss: 0.000188\n",
      "Epoch 205 | Train Loss: 0.000095 | Val Loss: 0.000139\n",
      "Epoch 206 | Train Loss: 0.000108 | Val Loss: 0.000074\n",
      "Epoch 207 | Train Loss: 0.000099 | Val Loss: 0.000095\n",
      "Epoch 208 | Train Loss: 0.000107 | Val Loss: 0.000062\n",
      "Epoch 209 | Train Loss: 0.000066 | Val Loss: 0.000143\n",
      "Epoch 210 | Train Loss: 0.000087 | Val Loss: 0.000066\n",
      "Epoch 211 | Train Loss: 0.000103 | Val Loss: 0.000062\n",
      "Epoch 212 | Train Loss: 0.000073 | Val Loss: 0.000068\n",
      "Epoch 213 | Train Loss: 0.000070 | Val Loss: 0.000073\n",
      "Epoch 214 | Train Loss: 0.000067 | Val Loss: 0.000049\n",
      "Epoch 215 | Train Loss: 0.000200 | Val Loss: 0.000061\n",
      "Epoch 216 | Train Loss: 0.000048 | Val Loss: 0.000046\n",
      "Epoch 217 | Train Loss: 0.000055 | Val Loss: 0.000055\n",
      "Epoch 218 | Train Loss: 0.000059 | Val Loss: 0.000050\n",
      "Epoch 219 | Train Loss: 0.000049 | Val Loss: 0.000039\n",
      "Epoch 220 | Train Loss: 0.000058 | Val Loss: 0.000050\n",
      "Epoch 221 | Train Loss: 0.000057 | Val Loss: 0.000038\n",
      "Epoch 222 | Train Loss: 0.000065 | Val Loss: 0.000095\n",
      "Epoch 223 | Train Loss: 0.000054 | Val Loss: 0.000101\n",
      "Epoch 224 | Train Loss: 0.000048 | Val Loss: 0.000036\n",
      "Epoch 225 | Train Loss: 0.000069 | Val Loss: 0.000034\n",
      "Epoch 226 | Train Loss: 0.000043 | Val Loss: 0.000068\n",
      "Epoch 227 | Train Loss: 0.000050 | Val Loss: 0.000032\n",
      "Epoch 228 | Train Loss: 0.000058 | Val Loss: 0.000078\n",
      "Epoch 229 | Train Loss: 0.000043 | Val Loss: 0.000034\n",
      "Epoch 230 | Train Loss: 0.000068 | Val Loss: 0.000033\n",
      "Epoch 231 | Train Loss: 0.000036 | Val Loss: 0.000029\n",
      "Epoch 232 | Train Loss: 0.000055 | Val Loss: 0.000037\n",
      "Epoch 233 | Train Loss: 0.000040 | Val Loss: 0.000127\n",
      "Epoch 234 | Train Loss: 0.000033 | Val Loss: 0.000026\n",
      "Epoch 235 | Train Loss: 0.000056 | Val Loss: 0.000079\n",
      "Epoch 236 | Train Loss: 0.000031 | Val Loss: 0.000023\n",
      "Epoch 237 | Train Loss: 0.000034 | Val Loss: 0.000084\n",
      "Epoch 238 | Train Loss: 0.000036 | Val Loss: 0.000059\n",
      "Epoch 239 | Train Loss: 0.000040 | Val Loss: 0.000040\n",
      "Epoch 240 | Train Loss: 0.000033 | Val Loss: 0.000024\n",
      "Epoch 241 | Train Loss: 0.000045 | Val Loss: 0.000019\n",
      "Epoch 242 | Train Loss: 0.000034 | Val Loss: 0.000018\n",
      "Epoch 243 | Train Loss: 0.000035 | Val Loss: 0.000017\n",
      "Epoch 244 | Train Loss: 0.000022 | Val Loss: 0.000016\n",
      "Epoch 245 | Train Loss: 0.000043 | Val Loss: 0.000017\n",
      "Epoch 246 | Train Loss: 0.000025 | Val Loss: 0.000015\n",
      "Epoch 247 | Train Loss: 0.000043 | Val Loss: 0.000016\n",
      "Epoch 248 | Train Loss: 0.000028 | Val Loss: 0.000017\n",
      "Epoch 249 | Train Loss: 0.000022 | Val Loss: 0.000040\n",
      "Epoch 250 | Train Loss: 0.000023 | Val Loss: 0.000046\n",
      "Epoch 251 | Train Loss: 0.000031 | Val Loss: 0.000012\n",
      "Epoch 252 | Train Loss: 0.000033 | Val Loss: 0.000023\n",
      "Epoch 253 | Train Loss: 0.000019 | Val Loss: 0.000012\n",
      "Epoch 254 | Train Loss: 0.000026 | Val Loss: 0.000013\n",
      "Epoch 255 | Train Loss: 0.000039 | Val Loss: 0.000032\n",
      "Epoch 256 | Train Loss: 0.000016 | Val Loss: 0.000024\n",
      "Epoch 257 | Train Loss: 0.000030 | Val Loss: 0.000044\n",
      "Epoch 258 | Train Loss: 0.000017 | Val Loss: 0.000012\n",
      "Epoch 259 | Train Loss: 0.000017 | Val Loss: 0.000014\n",
      "Epoch 260 | Train Loss: 0.000025 | Val Loss: 0.000020\n",
      "Epoch 261 | Train Loss: 0.000027 | Val Loss: 0.000011\n",
      "Epoch 262 | Train Loss: 0.000012 | Val Loss: 0.000019\n",
      "Epoch 263 | Train Loss: 0.000026 | Val Loss: 0.000011\n",
      "Epoch 264 | Train Loss: 0.000018 | Val Loss: 0.000013\n",
      "Epoch 265 | Train Loss: 0.000017 | Val Loss: 0.000063\n",
      "Epoch 266 | Train Loss: 0.000022 | Val Loss: 0.000013\n",
      "Epoch 267 | Train Loss: 0.000018 | Val Loss: 0.000016\n",
      "Epoch 268 | Train Loss: 0.000007 | Val Loss: 0.000007\n",
      "Epoch 269 | Train Loss: 0.000007 | Val Loss: 0.000007\n",
      "Epoch 270 | Train Loss: 0.000007 | Val Loss: 0.000007\n",
      "Epoch 271 | Train Loss: 0.000007 | Val Loss: 0.000007\n",
      "Epoch 272 | Train Loss: 0.000007 | Val Loss: 0.000007\n",
      "Epoch 273 | Train Loss: 0.000007 | Val Loss: 0.000006\n",
      "Epoch 274 | Train Loss: 0.000006 | Val Loss: 0.000008\n",
      "Epoch 275 | Train Loss: 0.000007 | Val Loss: 0.000006\n",
      "Epoch 276 | Train Loss: 0.000007 | Val Loss: 0.000008\n",
      "Epoch 277 | Train Loss: 0.000007 | Val Loss: 0.000006\n",
      "Epoch 278 | Train Loss: 0.000007 | Val Loss: 0.000007\n",
      "Epoch 279 | Train Loss: 0.000009 | Val Loss: 0.000007\n",
      "Epoch 280 | Train Loss: 0.000011 | Val Loss: 0.000007\n",
      "Epoch 281 | Train Loss: 0.000008 | Val Loss: 0.000008\n",
      "Epoch 282 | Train Loss: 0.000006 | Val Loss: 0.000014\n",
      "Epoch 283 | Train Loss: 0.000014 | Val Loss: 0.000006\n",
      "Epoch 284 | Train Loss: 0.000005 | Val Loss: 0.000005\n",
      "Epoch 285 | Train Loss: 0.000005 | Val Loss: 0.000005\n",
      "Epoch 286 | Train Loss: 0.000005 | Val Loss: 0.000005\n",
      "Epoch 287 | Train Loss: 0.000005 | Val Loss: 0.000005\n",
      "Epoch 288 | Train Loss: 0.000005 | Val Loss: 0.000005\n",
      "Epoch 289 | Train Loss: 0.000005 | Val Loss: 0.000005\n",
      "Epoch 290 | Train Loss: 0.000005 | Val Loss: 0.000005\n",
      "Epoch 291 | Train Loss: 0.000005 | Val Loss: 0.000005\n",
      "Epoch 292 | Train Loss: 0.000005 | Val Loss: 0.000005\n",
      "Epoch 293 | Train Loss: 0.000005 | Val Loss: 0.000005\n",
      "Epoch 294 | Train Loss: 0.000005 | Val Loss: 0.000005\n",
      "Epoch 295 | Train Loss: 0.000005 | Val Loss: 0.000005\n",
      "Epoch 296 | Train Loss: 0.000005 | Val Loss: 0.000005\n",
      "Epoch 297 | Train Loss: 0.000006 | Val Loss: 0.000012\n",
      "Epoch 298 | Train Loss: 0.000005 | Val Loss: 0.000009\n",
      "Epoch 299 | Train Loss: 0.000005 | Val Loss: 0.000004\n",
      "Epoch 300 | Train Loss: 0.000005 | Val Loss: 0.000004\n",
      "Epoch 301 | Train Loss: 0.000005 | Val Loss: 0.000004\n",
      "Epoch 302 | Train Loss: 0.000005 | Val Loss: 0.000005\n",
      "Epoch 303 | Train Loss: 0.000005 | Val Loss: 0.000005\n",
      "Epoch 304 | Train Loss: 0.000005 | Val Loss: 0.000004\n",
      "Epoch 305 | Train Loss: 0.000005 | Val Loss: 0.000004\n",
      "Epoch 306 | Train Loss: 0.000005 | Val Loss: 0.000006\n",
      "Epoch 307 | Train Loss: 0.000006 | Val Loss: 0.000004\n",
      "Epoch 308 | Train Loss: 0.000004 | Val Loss: 0.000004\n",
      "Epoch 309 | Train Loss: 0.000005 | Val Loss: 0.000004\n",
      "Epoch 310 | Train Loss: 0.000004 | Val Loss: 0.000004\n",
      "Epoch 311 | Train Loss: 0.000005 | Val Loss: 0.000004\n",
      "Epoch 312 | Train Loss: 0.000005 | Val Loss: 0.000005\n",
      "Epoch 313 | Train Loss: 0.000004 | Val Loss: 0.000007\n",
      "Epoch 314 | Train Loss: 0.000004 | Val Loss: 0.000006\n",
      "Epoch 315 | Train Loss: 0.000003 | Val Loss: 0.000004\n",
      "Epoch 316 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 317 | Train Loss: 0.000003 | Val Loss: 0.000004\n",
      "Epoch 318 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 319 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 320 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 321 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 322 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 323 | Train Loss: 0.000003 | Val Loss: 0.000004\n",
      "Epoch 324 | Train Loss: 0.000004 | Val Loss: 0.000004\n",
      "Epoch 325 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 326 | Train Loss: 0.000003 | Val Loss: 0.000004\n",
      "Epoch 327 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 328 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 329 | Train Loss: 0.000003 | Val Loss: 0.000004\n",
      "Epoch 330 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 331 | Train Loss: 0.000004 | Val Loss: 0.000003\n",
      "Epoch 332 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 333 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 334 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 335 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 336 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 337 | Train Loss: 0.000003 | Val Loss: 0.000004\n",
      "Epoch 338 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 339 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 340 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 341 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 342 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 343 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 344 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 345 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 346 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 347 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 348 | Train Loss: 0.000003 | Val Loss: 0.000004\n",
      "Epoch 349 | Train Loss: 0.000003 | Val Loss: 0.000003\n",
      "Epoch 350 | Train Loss: 0.000003 | Val Loss: 0.000003\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T23:18:06.163588Z",
     "start_time": "2026-02-23T23:18:06.147805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analytical_solution(u0_amplitudes, x, t, alpha=1.0):\n",
    "    u = np.zeros_like(x)\n",
    "    for k, A in enumerate(u0_amplitudes, start=1):\n",
    "        u += A * np.sin(k * np.pi * x) * np.exp(-alpha * (k * np.pi)**2 * t)\n",
    "    return u"
   ],
   "id": "c4d0b30597816d24",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T23:18:40.508052Z",
     "start_time": "2026-02-23T23:18:40.491445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "n_test = 100\n",
    "\n",
    "test_inputs = []\n",
    "test_amplitudes = []\n",
    "for _ in range(n_test):\n",
    "    u0, a = random_initial_condition(x)\n",
    "    test_inputs.append(u0)\n",
    "    test_amplitudes.append(a)\n",
    "\n",
    "test_tensor = tc.tensor(np.array(test_inputs), dtype=tc.float32).unsqueeze(-1)"
   ],
   "id": "915cac8e5a9d9890",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T00:13:10.690627Z",
     "start_time": "2026-02-24T00:13:08.671345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# scipy\n",
    "start = time.time()\n",
    "for u0 in test_inputs:\n",
    "    solve_heat_equation(u0, x)\n",
    "scipy_time = time.time() - start\n",
    "\n",
    "# FNO\n",
    "start = time.time()\n",
    "with tc.no_grad():\n",
    "    model(test_tensor)\n",
    "FNO_time = time.time() - start\n",
    "\n",
    "print(\n",
    "    f\"Scipy: {scipy_time:.3f}s | FNO: {FNO_time:.3f}s | Speedup: {scipy_time / FNO_time:.1f}x\")\n"
   ],
   "id": "46955dbb02f252e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scipy: 1.999s | FNO: 0.010s | Speedup: 208.0x\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T00:13:27.448283Z",
     "start_time": "2026-02-24T00:13:27.419478Z"
    }
   },
   "cell_type": "code",
   "source": "FNO_solution = model(test_tensor).detach().numpy()",
   "id": "5861d1fb717c2b4a",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T23:55:40.901311Z",
     "start_time": "2026-02-23T23:55:40.883790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ana_solution = np.zeros_like(GNO_solution)\n",
    "for i in range(100):\n",
    "    ana_solution[i,:] = analytical_solution(test_amplitudes[i], x, t = 0.1)"
   ],
   "id": "e853f052267b2e43",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T00:13:33.847990Z",
     "start_time": "2026-02-24T00:13:33.822148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "relative_l2 = 0\n",
    "for i in range(100):\n",
    "    relative_l2 += np.sqrt(np.sum((FNO_solution[i, :] - ana_solution[i, :]) ** 2)) / np.sqrt(\n",
    "        np.sum(ana_solution ** 2)) / 100\n",
    "print(relative_l2*100)"
   ],
   "id": "bcb8629f20e29dd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11325549\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T23:59:11.462349Z",
     "start_time": "2026-02-23T23:59:11.443295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_test_128 = np.linspace(0, 1, 128)\n",
    "\n",
    "test_inputs_128 = []\n",
    "test_amplitudes_128 = []\n",
    "for _ in range(n_test):\n",
    "    u0, a = random_initial_condition(x_test_128)\n",
    "    test_inputs_128.append(u0)\n",
    "    test_amplitudes_128.append(a)\n",
    "\n",
    "test_tensor_128 = tc.tensor(np.array(test_inputs_128), dtype=tc.float32).unsqueeze(-1)"
   ],
   "id": "94f6351a97e53bad",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T00:13:38.147847Z",
     "start_time": "2026-02-24T00:13:38.110460Z"
    }
   },
   "cell_type": "code",
   "source": "FNO_solution_128 = model(test_tensor_128).detach().numpy()",
   "id": "c1abf715c3ef340d",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T00:00:19.316248Z",
     "start_time": "2026-02-24T00:00:19.307695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ana_solution_128 = np.zeros_like(GNO_solution_128)\n",
    "for i in range(100):\n",
    "    ana_solution_128[i,:] = analytical_solution(test_amplitudes_128[i], x_test_128, t = 0.1)"
   ],
   "id": "b0a9b6d2e81bfa4f",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-24T00:13:44.167829Z",
     "start_time": "2026-02-24T00:13:44.139231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "relative_l2_128 = 0\n",
    "for i in range(100):\n",
    "    relative_l2_128 += np.sqrt(np.sum((FNO_solution_128[i, :] - ana_solution_128[i, :]) ** 2)) / np.sqrt(\n",
    "        np.sum(ana_solution_128 ** 2)) / 100\n",
    "print(relative_l2_128*100)"
   ],
   "id": "fcae4a257360d49f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1806133\n"
     ]
    }
   ],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
